# InternLM2 技术报告笔记

## 1. 基本内容

 InternLM2是一个开源 LLM，通过创新的预训练和优化技术，在综合评估、长期上下文建模和开放式主观评估方面优于之前的模型。InternLM2 的预训练过程非常详细，详细说明了各种数据类型的准备，包括文本、代码和长上下文数据。InternLM2 有效地捕获了长期依赖关系，最初在 4k token上训练，然后在预训练和微调阶段推进到 32k token，并在 200k“大海捞针”测试中表现出卓越的性能。

 InternLM2 采用分组查询注意力 （GQA） 来在推断长序列时实现更小的内存占用。

在预训练阶段，首先使用 4k 长度的上下文文本训练 InternLM2，然后将训练语料库转换为高质量的 32k 长度的文本以进行进一步训练。之后利用监督微调 （SFT） 和来自人类反馈的强化学习 （RLHF） 来确保模型很好地遵守人类指令并与人类价值观保持一致。

主要贡献包括：

- **性能强大的开源 InternLM2**：模型规模包括 1.8B、7B和20B，每个大小的模型还发布了不同的版本，包括Base模型、SFT微调模型、RLHF微调模型。
- **使用200k上下文窗口**：表现出令人印象深刻的长上下文性能。

- **全面的数据准备指南**：我们详细阐述了 LLM 的数据准备，包括预训练数据、特定领域的增强数据、SFT 数据和 RLHF 数据。
- **RLHF训练**：使用在线RLHF来协调各种偏好，显着提高了 InternLM2 在各种主观对话评估中的表现。

## 2. 训练基础架构

### 2.1 训练框架 InternEvo

 InternEvo 是一个高效且轻量级的预训练框架，可以用来训练大模型。该框架能够在数千个 GPU 上扩展模型训练。这是通过数据、张量、序列和流水线并行的组合来实现的。为了进一步提高 GPU 内存效率，InternEvo 集成了各种零冗余优化器 （ZeRO）策略，显著减少训练所需的内存占用。此外，为了提高硬件利用率，我们使用了 FlashAttention 和混合精度训练与BF16。

InternEvo 在数千个 GPU 上训练 InternLM 时表现出强大的扩展性能。在 8 个 GPU 上训练 InternLM-7B 时，全局批量大小为 400 万个 token，InternEvo 实现了 64% 的模型 FLOP 利用率 （MFU）。将训练扩展到 1024 个 GPU，InternEvo 在相同的全局批处理大小下保持了 53% MFU。

InternEvo 还表现出强大的序列长度缩放，在训练不同大小的 LLM 时支持 256,000 个 token。当训练序列长度为256,000 token 时，InternEvo 可以实现近 88% 的 MFU。

#### 减少通信开销

InternEvo 通过使用一套自适应分片技术来实现强大的扩展性能，从而解决通信挑战。其中包括全副本、全分片和部分分片，它们允许模型的每个组件（参数、梯度和优化器状态）独立选择最合适的分片方法和设备组网配置。

#### 通信-计算重叠

InternEvo 进一步降低了通信开销，协调通信和计算，以优化整体系统性能。使用参数分片时，模型的参数分布在多个 GPU 上以节省 GPU 显存。在训练过程中每个 micro-batch 的每次正向和后向传递期间，InternEvo 在计算当前层时通过 AllGather 预取即将到来的层的完整参数。生成的梯度通过 ReduceScatter 在参数分片组内进行同步，随后使用 AllReduce 在参数分片组之间同步。这些通信过程巧妙地与反向计算重叠，从而最大限度地提高了训练管道的效率。在优化器状态分片的情况下，GPU 通过 Broadcast 广播分片组内的更新参数，InternEvo 采用与下一个训练步骤的前向计算的策略重叠。这些创新的重叠方法有效地平衡了通信开销和计算执行时间，从而显著提高了整体系统性能。

#### 长序列训练

长序列训练的主要挑战之一是计算速度和通信开销之间的权衡。InternEvo 将 GPU 内存管理分解为具有四个并行维度（数据、张量、序列和流水线）和三个分片维度（参数、梯度和优化器状态）的分层空间。对每个维度的内存和通信成本进行全面分析，利用执行模拟器来识别和实施最佳并行化策略。InternEvo 还实现了内存管理技术，以减少 GPU 内存碎片。

#### 容错

我们对 GPU 数据中心为期六个月的 LLM 开发工作负载跟踪进行了深入的表征研究。本研究确定了 LLM 与以前的深度学习工作负载之间的差异，并探讨了资源利用模式和作业失败的影响。基于我们的分析，我们引入了两个系统工作：一个是容错预训练系统，它通过涉及LLM的故障诊断和自动恢复来增强容错能力，另一个是用于评估任务的解耦调度系统，该系统提供及时的模型性能反馈。

#### 2.2 交互式训练

InternEvo 的效率在人类反馈强化学习 （RLHF） 阶段也得到了成功证明，该阶段部署了多个 LLM 进行交互式训练。例如，在近端策略优化 （PPO） 过程中，我们利用四个大小相等的模型并训练其中两个；InternEvo 使每个模型都能以最佳配置执行。为了加强多个模型的协调，我们开发了一个基于 InternEvo 和 Ray 的创新 RLHF 框架。该框架的特点是其灵活性和可扩展性，使其能够大规模有效地执行。

### 模型结构

选择坚持LLaMA的结构设计原则。为了提高效率，我们修改了 $W_q$、$W_k$、$W_v$​ 矩阵布局，在矩阵的最后一个维度拆分或拼接矩阵来实现 tensor parallel 大小的调整，从而增强了模型在不同分布式计算环境中的灵活性。

<img src="InternLM2_tech_report.assets/weight matrix layouts.png" alt="image-20240328092147423" style="zoom：67%；" />

## 3. 预训练

### 3.1 预训练数据

本节将描述用于准备通用文本数据、编程语言相关数据和长文本数据的数据处理管道。

#### 3.1.1 文本数据

预训练数据集中的文本数据可以按来源分类为网页、论文、专利和书籍。为了将这些源转换为预训练数据集，首先将所有数据标准化为指定的格式，按类型和语言对它们进行分类，并以 JSON 行 （jsonl） 格式存储它们。然后，对于所有数据，应用几个处理步骤，包括基于规则的过滤、重复数据删除、安全过滤和质量过滤。最终产生丰富、安全和高质量的文本数据集。

##### 数据源分布

自网页的中英文数据占总数的86.46%，是主要来源。其他来源的数据量相对较小，例如书籍和技术文献（缩写为techlit），但平均文档长度更长，内容质量相对较高，因此同样重要。

<img src="InternLM2_tech_report.assets/text data.png" alt="image-20240328092714566" style="zoom： 67%；" />

##### 数据处理管道

整个数据处理流水线首先对来自不同来源的数据进行标准化，以获得**格式化数据**。然后，使用启发式统计规则进行数据过滤，以获得**干净的数据**。接下来，使用局部敏感哈希（LSH）方法进行重复数据删除，以获取**重复数据删除数据**。然后，我们应用复合安全策略来过滤数据，从而产生**安全数据**。我们对来自不同来源的数据采用了不同的质量过滤策略，最终获得了**高质量的预训练数据**。

<img src="InternLM2_tech_report.assets/Data Process Pipeline.png" alt="image-20240328093117086" style="zoom： 67%；" />

#### 3.1.2 代码

##### 数据源分布

<img src="InternLM2_tech_report.assets/code data.png" alt="image-20240328093253492" style="zoom：50%；" />

我们从各种来源收集数据，包括从 GitHub 直接抓取、公共数据集以及与代码和编程相关的在线资源，如问答论坛、教程网站和 API 文档。

##### 格式

所有数据都转换为统一的 Markdown 格式。不过一小部分数据仍然表现出损坏的HTML或XML格式，我们使用了一组启发式规则来最大程度地减少这些事件的发生。之所以选择 Markdown，是因为它简单明了，最大限度地减少了格式化的 token 开销，并且兼容代码和自然语言的相互交错。

##### 重复数据删除

我们的见解是，有效的分词器对于应用通用重复数据删除策略至关重要。尽管最近的研究在段落或行级别探索了细粒度重复数据删除，但我们的方法仍然停留在文件级别，以保持上下文的完整性。

##### 质量过滤

<img src="InternLM2_tech_report.assets/code quality classifier pipeline.png" alt="image-20240328093948466" style="zoom：50%；" />

数据质量是LLM研究中预训练的一个关键但模糊的方面，主要是因为难以量化其对模型性能的影响。我们采用了混合的多阶段过滤过程，包括基于规则和模型的评分器。基于规则的评分器是启发式的和多样化的。对于基于模型的评分，我们评估了几个骨干模型，用大约 50,000 个样本训练它们。不过，我们观察到，评分模型评估与人类判断之间的相关性因语言而异，扩大训练集并不能显着提高评分模型的准确性。因此，我们只对模型预测与人工注释的验证集上的人工评估非常一致的语言采用基于模型的评分。

<img src="InternLM2_tech_report.assets/code quality.png" alt="image-20240328093611498" style="zoom： 67%；" />

我们根据训练好的评分模型对代码数据质量进行评估。高质量的数据将具有更高的采样权重，并且可以在预训练阶段进行多次训练迭代。中等质量的数据具有正常的采样权重，通常训练一次。低质量的数据被排除在外，因为我们的实证研究结果证实，尽管它们的比例相对较小，但删除它们对于优化模型性能和确保训练稳定性至关重要。

#### 3.1.3 长上下文数据

##### 数据过滤管道

我们的数据处理管道旨在过滤掉低质量的长文本数据。它包括三个阶段：

1. 长度选择，一个基于规则的过滤器，选择超过32K字节的数据样本；
2. 统计过滤器，利用统计特征来识别和删除异常数据；
3. 困惑度过滤器，它利用困惑度的差异来评估文本片段之间的连贯性，过滤掉具有分散注意力的上下文的样本。

整个过滤过程消除了很大一部分网页数据和专利，而大多数书籍和纸张数据则被保留下来。

### 3.2 预训练设置

#### 3.2.1 Tokenization

我们选择 GPT-4 的 tokenization 方法，因为它在压缩各种文本内容方面具有非凡的效率。我们的主要参考资料是 cl100k 词汇表，主要包括英语和编程语言 token，共计 100,256 个条目，少量包含不到 3,000 个中文 token。为了优化 InternLM 在处理中文文本时的压缩率，同时将整体词汇量保持在 100,000 以下，我们从 cl100k 词汇表中精心挑选了前 60,004 个 token，并将它们与 32,397 个中文 token 集成在一起。此外，我们还包括 147 个备用 token 来完善选择，最终词汇量与 256 的倍数一致，从而促进了高效的训练。

#### 3.2.2 预训练 Hyper-parameters

| Params | $n_{layers}$ | $n_{dim}$ | $n_{kv\_heads}$ | $n_{q\_per\_head}$ | Learning Rate | Batch size |
| :----- | :----------: | :-------: | :-------------: | :----------------: | :-----------: | :--------: |
| 1.8B   |      24      |   2048    |        8        |         2          |     3e-4      |     4M     |
| 7B     |      32      |   4096    |        8        |         4          |     3e-4      |     4M     |
| 20B    |      48      |   6144    |        8        |         6          |     3e-4      |     5M     |

在训练期间，使用AdamW，超参数为 $\beta_1 = 0.9$、$\beta_2 = 0.95$、$\epsilon=1e-8$、$weight\_decay=0.1$。使用余弦学习率衰减，学习率衰减到最大值的 10%。$\beta_1$、$\beta_2$、$weight\_decay=0.1$、余弦学习率衰减和llama2 一样，不过这里的 $\epsilon=1e-8$ 使用的是默认值，而 Llama2 使用的是 $\epsilon=1e-5$，更大一些。

### 3.3 预训练阶段

用于预训练 1.8B、7B 和 20B 模型的 token 总数从 2.0T 到 2.6T 不等，预训练过程由三个不同的阶段组成。在第一阶段，我们使用了长度不超过 4k 的预训练语料库。在第二阶段，我们纳入了 50% 长度不超过 32k 的预训练语料库。在第三阶段，我们利用了特定于功能的增强数据。在每个阶段，我们混合了英文、中文和代码的数据。

#### 3.3.1 4k 长度上下文训练

对于大约 90% 的训练步骤，我们使用长度为 4096 个 token 的数据进行训练。如果数据长度超过 4096，我们将强制截断它，剩余的部分也用于训练。

#### 3.3.2 长上下文训练

我们对 InternLM2 的训练过程从 4K 长度的上下文语料库开始，然后过渡到具有 32K 长度的上下文的语料库。50%的数据仍然短于4096个 token，而不是只使用32k语料库。这个长上下文训练阶段约占总步骤的 9%。为了适应这些较长的序列，我们将旋转位置嵌入 （RoPE） 基数从 50,000 调整到 1,000,000，确保对长上下文进行更有效的位置代码。由于 InternEvo 的良好的可扩展性以及使用了 Flash Attention，当将上下文窗口从 4K 更改为 32K 时，训练速度仅降低 40%。

#### 3.3.3 指定能力增强训练

推理、数学问题解决和知识记忆等能力是大型语言模型应具备的关键能力。然而，在预训练过程中，高质量的能力相关数据在整个语料库中分布稀疏，这使得模型很难精通这些能力。

在 InternLM2 中，我们收集了一个丰富的数据集，其中包含高质量检索数据和来自 Huggingface 的各种类型的开源数据集。我们在这个数据集中总共收集了 240 亿个 token。我们过滤掉与测试集相关的数据。为了使模型很好地拟合这些数据，我们采用了较小的学习率和批量大小。

在此增强训练阶段之后，InternLM2 模型在编码、推理、问答和考试方面表现出显着的性能改进。

## 4. 对齐

### 4.1 监督微调

在监督微调 （SFT） 阶段，我们使用了 1000 万个指令实例的数据集，这些实例已经过筛选，以确保它们的有用性和无害性。该数据集涵盖了广泛的主题，包括一般对话、NLP 任务、数学问题、代码生成和函数调用等。为了便于多功能地表示这些不同的任务，我们将数据样本转换为ChatML格式。7B 和 20B 模型都使用 AdamW 优化器进行一个 epoch 的训练，初始学习率为 4e-5。

<img src="InternLM2_tech_report.assets/The distribution of SFT data instances.png" alt="image-20240328111313142" style="zoom: 50%;" />

### 4.2 COOL RLHF

从人类反馈中强化学习 （RLHF）是大型语言模型领域的一种创新方法。通过结合人类反馈，RLHF 创建了奖励模型，作为人类偏好的 Agent ，从而为 LLM 提供奖励信号，以便通过使用近端策略优化 （PPO） 进行学习这种方法使模型能够更好地理解和执行传统方法难以定义的任务。

尽管RLHF取得了一些成就，但其实际应用仍然存在一些问题。首先是偏好冲突。例如，在开发对话系统时，我们希望它能提供有用的信息（有帮助），同时不会产生有害或不适当的内容（无害）。然而，在实践中，这两种偏好往往不能同时满足，因为在某些情况下，提供有用的信息可能涉及敏感或高风险的内容。

为了解决这些问题，我们提出了 **Co**nditional **O**n **L**ine RLHF （COOL RLHF）。COOL RLHF首先引入了条件奖励机制来调和不同的偏好，该机制允许奖励模型根据特定条件动态地将注意力分配给各种偏好，从而优化整合多个偏好。此外，COOL RLHF采用多轮在线RLHF策略，使LLM能够迅速适应新的人类反馈，从而减少 reward hacking 的发生。

#### 4.2.1 条件奖励模型

条件奖励模型代表了一种创新的解决方案，可以解决以前 RLHF 方法偏好建模中固有的挑战。与通常依靠多个偏好模型来解决不同领域的偏好冲突的传统方法不同，条件奖励模型结合了针对不同类型偏好的不同系统提示，以有效地在单一奖励模型中对各种偏好进行建模。

<img src="InternLM2_tech_report.assets/Architecture of the Conditional Reward Model.png" alt="image-20240328112602794" />

（a） LLaMA2采用不同的奖励模型来解决偏好冲突问题。（b）我们所提出的条件奖励模型利用条件系统提示来协调各个领域的偏好数据，从而能够使用单一奖励模型对多个偏好进行建模。

条件奖励模型采用不同的系统提示来无缝混合来自各个领域的数据。由于奖励模型是从SFT模型初始化的，SFT模型已经学会了遵循不同的人类指令，我们也让奖励模型遵循不同的系统提示，以适应不同场景的不同偏好。在条件奖励模型中，系统提示不仅仅是其输入的一个组成部分，它们也是在不同场景中根据特定偏好指导奖励分数的重要工具。这种集成有助于在不牺牲准确性的情况下，在统一的奖励模型中管理相互矛盾和复杂的人类偏好。

##### 数据构成

条件奖励模型的训练过程涉及广泛的数据集，涵盖对话、文章写作、诗歌、总结、代码、数学和格式化输出等各个领域，拥有多达 240 万个二分偏好对。这个全面的数据集确保了模型的广泛适应性，并增强了其在更广泛和更复杂的情况下进行强化学习的能力。因此，通过采用条件系统提示方法，奖励模型可以响应复杂的人类需求，从而在PPO阶段对奖励分数进行更细致的控制。

##### 损失函数

此外，为了减少数据集中易难样本不平衡的影响，受到 Focal Loss 启发，我们修改了原有的排名损失函数。我们在排名损失中添加了难度衰减系数，使困难样本的损失值较大，简单样本的损失值较小，从而防止了大量简单样本的过度拟合。Focal 排名损失表述为：


$$
L_{ranking} = - (1 - 2 \times max(0, P_{i,j} - \frac 1 2))^{\gamma} \ \text{log}(P_{i,j})),
$$


其中 $P_{i,j} = \sigma(r_i - r_j)$ 表示 $reward_i$ 大于 $reward_j$ 的概率。难度衰减系数仅在模型正确预测训练样本的偏好时才生效，即 $P_{i,j} > 0.5$​，否则等于 1。

$\gamma$​ 代表一个超参数，有助于调节难度衰减比。这里我们默认设置为2。同时，为了确保奖励模型在不同训练中输出分数的稳定性和一致性，我们对奖励分数引入对数障碍惩罚，将分数分布限制在-5到5的范围内，定义为：


$$
L_{penalty} = -(log(x + 5) + log(5 - x))
$$


这种限制至关重要，因为它避免了在 PPO 阶段修改其他与奖励相关的超参数，这可能是由于不同奖励模型中奖励分数分布的变化而产生的。总体而言，奖励模型的损失函数为：


$$
L = L_{ranking} + \lambda L_{penalty}
$$


参数 $\lambda$ 是一个加权系数，用于平衡 $L_{ranking}$ 和 $L_{penalty}$​ 的贡献。我们根据初步实验结果的观察将 0.02 设置为默认值。它提高了奖励模型的稳健性和一致性，特别是在简单样本和困难样本之间数量不平衡的背景下。

##### 训练细节

在我们的实验中，我们将奖励模型的大小与PPO中使用的模型的大小保持一致。遵循 InstructGPT 中描述的方法，使用 SFT 模型权重初始化奖励模型，将输出层修改为随机初始化的一维线性映射层。我们的批处理构建策略侧重于将偏好数据的总长度固定为每批 16384 个 token，而不是限制偏好对的数量，以避免由于数据填充而导致的训练效率低下。最大上下文长度设置为 8192。每个序列的末尾都会附加一个特殊的标记，其输出值用作奖励分数。我们采用 AdamW 作为优化器。学习率遵循余弦退火策略，从 1e-5 降低到 5e-6，权重衰减设置为 0.01。为了防止过度拟合，模型训练了一个 epoch。

#### 4.2.2 在线 RLHF

在获得条件奖励模型后，我们进行近端策略优化（PPO），用来让LLM与奖励模型建模的人类偏好保持一致。为了应对 PPO 阶段的 reward hacking 攻击挑战，我们引入了一种在线 RLHF 方法，分为两种不同的方式：一种是用于即时、有针对性的改进的快速路径，另一种是用于长期、全面改进奖励模型的慢速路径。快速路径和慢速路径是互补的，它们提供了一个自适应框架，用于减少 reward hacking 攻击，并提高根据人类反馈训练的 LLM 的性能和可靠性。

##### 快速路径

在线RLHF的快速路径侧重于通过有针对性的补丁快速识别和纠正 reward hacking 事件，以提高奖励模型的可靠性。随着 PPO 训练的进行，LLM 被鼓励朝向跟高奖励区域，这些区域通常会暴露更多容易被发现的 reward hacking 场景。在确定每轮 RLHF 后的hacking 模式后，我们通过比较当前轮中早期和晚期 PPO 模型生成的响应来构建突出这些模式的偏好对。在训练过程中加入 20 到 100 个这样的偏好对就足以防止奖励模型受到相应的 hacking 攻击。这个过程可以快速修复奖励模型以应对新出现的 hacking 行为，提高奖励模型的可靠性和对预期结果的遵守。

##### 慢速路径

使用模型在训练的各个阶段（包括 SFT 模型、早期 PPO 模型和后期 PPO 模型）生成的响应来形成成对比较。然后将这些对呈现给专业的人类注释者，以标记他们的偏好。

这样的过程对奖励模型进行了更细致和彻底的改进，但需要大量的人工注释时间。为了提高在线RLHF的效率，我们只使用所有先前模型在实验启动时累积的人类偏好。通过根据人类反馈不断更新模型，慢路径确保奖励模型与人类偏好的复杂性和微妙之处同步发展。

##### 实现

在实施在线RLHF的过程中，我们进行了三轮细化。在这些周期中，我们在快速路径中收集了数千个偏好补丁和在线偏好数据，以更新奖励模型，同时使用了以前模型中所有现有的人类偏好数据。

#### 4.2.3 PPO 训练详情

在RL对齐阶段，我们采用了标准的PPO（近端策略优化）算法，并对其进行了多次调整，确保了更稳定的训练过程。该框架涉及四种模型：actor 模型、 critic 模型、 reference 模型和奖励模型。在训练过程中，后 2 个模型被冻结，只有前 2 个模型被主动训练。值得注意的是，所有这些模型都具有相同的大小，确保了它们处理和生成数据的能力的一致性。我们在大约 400 次迭代中遍历了大约 200k 个不同的查询，并在要发布的验证集结果上选择最佳检查点。

##### 模型初始化

我们从 SFT 模型权重初始化 reference 模型和 actor 模型。critic 模型是从奖励模型（不包括线性头）初始化的，并经过 50 次迭代的预训练阶段，在此期间，actor 模型被冻结。此阶段对于稳定早期训练中的值估计至关重要，从而防止不稳定值的潜在不利影响。我们进行了消融实验，比较 critic 模型使用奖励模型或SFT模型作为初始化的差异。我们的结果表明，在PPO训练的前几次迭代中，从奖励模型初始化的 critic 模型表现出较大的损失，但在大约20次迭代后，它始终表现出较低的损失，并为 actor 模型带来了更高的奖励。我们假设，在初始阶段观察到的较高损失可能揭示了奖励建模和批评建模任务之间的根本差异。随后的损失减少可归因于模型内部对世界知识的理解更加一致，和对评估原则的掌握也更加准确。

![image-20240328151010844](InternLM2_tech_report.assets/image-20240328151010844.png)

##### 条件奖励

我们的奖励模型经过训练以适应各种条件。对于来自不同领域的查询，在计算奖励分数之前为每个样本响应添加适当的条件系统提示。确保模型的响应在上下文中与不同领域的不同需求保持一致。

<img src="InternLM2_tech_report.assets/image-20240328152153980.png" alt="image-20240328152153980" style="zoom:50%;" />

##### 预训练梯度

为了降低 PPO 阶段发生灾难性遗忘的风险，我们遵循 InstructGPT 方法，加入了预训练损失。预训练损失系数设置为0.5，预训练数据量约为PPO训练数据量的50%。这一补充有助于保留在初始训练阶段获得的知识，确保模型保留其基础能力和知识库，同时适应新的反馈和通过 PPO 学习。

##### 超参数

我们将 KL 散度系数设置为 0.01。actor 模型和 critic 模型的学习率分别设置为 1e-6 和 5e-6。在我们的实验中，我们发现更大的 $\lambda$ 会给 PPO 带来更高的奖励，因此我们将其设置为 0.99。我们采用了一种稍微保守的抽样策略，其中 $top\_p=0.9$​，在采样多样性和收敛速度之间取得平衡。与一些传统方法不同，我们不应用梯度剪裁或高级标准化。

### 4.3 长上下文微调

为了在微调后保留LLM的长上下文能力，我们继续使用SFT和RLHF中的长上下文预训练数据，这是受之前在SFT中采用长上下文预训练语料库的工作的启发。具体来说，我们使用两种类型的数据：一种是来自书籍的长上下文数据，另一种是从 GitHub 存储库获取并通过特定范式连接起来的长上下文数据。

为了增强 InternLM2 的数据分析能力，我们选择了 DS-1000 中使用的代码库作为核心存储库，包括 Pandas、Numpy、Tensorflow、Scipy、Scikit-learn、PyTorch 和 Matplotlib。然后，我们在 GitHub 上搜索具有超过 10,000 颗星的存储库，这些存储库引用了这些核心存储库，并执行了与预训练相同的过滤和数据清理过程。对于每个存储库，我们最初使用深度优先的方法对获取的原始数据进行排序，同时生成简要描述文件内容所需的提示。随后，我们按顺序拼接处理后的数据，直到达到 32k 的长度。实验结果表明，长上下文代码数据不仅提高了LLM的长上下文能力，还提高了代码能力。

<img src="InternLM2_tech_report.assets/Illustration of the process to obtain long-context code data.png" alt="image-20240328141139726" style="zoom： 80%;" />

### 4.4 使用工具

我们采用 ChatML 格式的修改版本，通过引入“环境”角色来实现通用工具调用。这种修改在聊天场景中共享相同的格式，但在采用 Agent 时为模型提供更清晰的信号。此外，我们还定义了两个特定的关键字来支持 AI Agent 的不同用途，即代码解释器 （<|interpreter|>） 和外部插件 （<|plugin|>）。这使我们能够采用统一的流式格式，可以处理各种类型的插件扩展和 AI 环境，同时与一般聊天兼容。为了充分激发 InternLM2 的智能体能力，我们将智能体语料库与聊天域对齐，并沿着语言模型的基本功能进行细粒度训练。

## 5 评估与分析

### 5.2 下游任务能力

#### 5.2.1 综合性能

对于基本型号，InternLM2 系列在具有相似参数数量的型号中表现良好。在 7B 和 20B 上，InternLM2 相比 InternLM2-Base 有着显着的性能增长，这证明了对通用领域数据和领域增强语料库的预训练对于综合性能测试具有优势。对于专门从人类设计的考试中收集的 AGIEval 和 GAOKAO 任务，InternLM2 相对于 InternLM2-Base 展现出更大的性能优势（比其他数据集的优势更大）。

对于聊天模型，InternLM2 系列在具有相似参数数量的模型中也表现出色。通过比较 InternLM2-Chat-7B-SFT 和 InternLM2-Chat-7B 模型，可以看出 COOL RLHF 对综合性能的影响很小。

#### 5.2.2 语言能力和知识

得益于其严谨和高质量的训练语料库，InternLM2 在涉及语言理解和知识应用的任务中表现出了显着的性能优势。因此，它成为众多实际应用程序的绝佳选择，在这些应用程序中，模型拥有强大的语言理解能力以及拥有广泛的知识至关重要。

#### 5.2.3 推理/数学

##### 推理

对于7B左右的模型，InternLM2-7B 在 BBH 以外的大多数数据集上表现出卓越的性能。在所有测试的基本型号中，InternLM2-20B 的整体性能最佳。与 InternLM2-20B-Base 相比，领域增强知识的加入显著提高了常识推理能力，InternLM2-20B 在测试的推理数据集中相比 Base 模型的平均分数增加了 10.4%。

##### 数学

在7B参数的模型中，InternLM2-7B 处于领先地位。在 13∼20B 参数的模型中，InternLM2-20B 在基本算术和定理证明方面优于所有测试的 Base 模型，而Qwen-14B-Base 在解决问题任务和 MathBench 的中英文测试中都表现出色。

#### 5.2.4 代码

InternLM2 系列拥有领先的性能，尤其是在 HumanEval、MBPP 和 MBPP-CN 上，其中 InternLM2-Chat-20B 型号比以前最先进的型号高出 10% 以上，突显了 InternLM2 系列在代码生成任务方面的熟练程度。此外，InternLM2-Chat-20B 模型在 MBPP-CN 基准测试中比 InternLM2-Chat-7B 模型有显著改进，但在 HumanEval-X 上的性能略有下降。这种现象可能源于 InternLM2-Chat-20B 模型针对中文进行了微调，而牺牲了其在其他语言中的有效性。

### 5.3 对齐性能

尽管 LLM 的客观能力通过预训练和 SFT 得到改善，但他们的回答风格可能与人类的偏好不一致，因此需要进一步增强 RLHF 以提高对齐性。因此，评估对齐能力对于确定 LLM 是否真正满足人类需求至关重要。

InternLM2在对齐任务中的整体性能在多个基准测试中达到了SOTA或接近SOTA的结果，表明 InternLM2 系列模型的主观输出与人类偏好之间具有高度的一致性。

## 6. 结论

在本报告中，我们介绍了 InternLM2 大语言模型，该模型在主观和客观评估中都表现出出色的性能。InternLM2 接受了超过 2T 的高质量预训练语料库的训练，涵盖 1.8B、7B 和 20B 的模型大小，适用于多种场景。为了更好地支持长上下文，InternLM2 使用 GQA 来降低推理成本，并在多达 32k 的上下文上进行了额外训练。除了开源模型本身外，我们还提供了训练过程各个阶段的检查点，以促进未来的研究。

除了开源模型之外，我们还详细介绍了我们如何训练 InternLM2，包括训练框架、预训练文本数据、预训练代码数据、预训练长文本数据和对齐数据。此外，为了解决 RLHF 过程中遇到的偏好冲突，我们提出了基于条件的在线 RLHF 来协调各种偏好。这些信息可以提供有关如何准备预训练数据以及如何更有效地训练大型模型的见解。