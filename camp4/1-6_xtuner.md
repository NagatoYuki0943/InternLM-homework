# ä½¿ç”¨ XTuner å¾®è°ƒ InternLM2-Chat-7B å®ç°è‡ªå·±çš„å°åŠ©æ‰‹è®¤çŸ¥

## å‡†å¤‡ç¯å¢ƒ

ç”±äºæˆ‘ä¹‹å‰å·²ç»åœ¨ openxlab ä¸Šè£…å¥½äº†ç¯å¢ƒï¼Œå› æ­¤åªéœ€è¦æŸ¥çœ‹ç¯å¢ƒæ˜¯å¦æ­£å¸¸å³å¯ã€‚

```sh
xtuner list-cfg
```

æ­£å¸¸è¾“å‡º

![image-20241031200917198](1-6_xtuner.assets/image-20241031200917198.png)

## å‡†å¤‡æ•°æ®

åˆ›å»ºä¸€ä¸ªæ–°çš„æ–‡ä»¶å¤¹ç”¨äºå­˜å‚¨å¾®è°ƒæ•°æ®

```sh
git clone https://github.com/InternLM/Tutorial.git -b camp4
mkdir -p /root/finetune/data && cd /root/finetune/data
cp -r /root/Tutorial/data/assistant_Tuner.jsonl  /root/finetune/data
```

![image-20241031201049003](1-6_xtuner.assets/image-20241031201049003.png)

åˆ›å»ºä¿®æ”¹è„šæœ¬

æˆ‘ä»¬å†™ä¸€ä¸ªè„šæœ¬ç”Ÿæˆä¿®æ”¹æˆ‘ä»¬éœ€è¦çš„å¾®è°ƒè®­ç»ƒæ•°æ®ï¼Œåœ¨å½“å‰ç›®å½•ä¸‹åˆ›å»ºä¸€ä¸ª `change_script.py` æ–‡ä»¶ï¼Œå†…å®¹å¦‚ä¸‹ï¼š

```sh
# åˆ›å»º `change_script.py` æ–‡ä»¶
touch /root/finetune/data/change_script.py
```

æ‰“å¼€è¯¥`change_script.py`æ–‡ä»¶åå°†ä¸‹é¢çš„å†…å®¹å¤åˆ¶è¿›å»ã€‚

ä¿®æ”¹åå­—ä¸ºè‡ªå·±çš„åå­—ã€‚

```python
import json
import argparse
from tqdm import tqdm

def process_line(line, old_text, new_text):
    # è§£æ JSON è¡Œ
    data = json.loads(line)

    # é€’å½’å‡½æ•°æ¥å¤„ç†åµŒå¥—çš„å­—å…¸å’Œåˆ—è¡¨
    def replace_text(obj):
        if isinstance(obj, dict):
            return {k: replace_text(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [replace_text(item) for item in obj]
        elif isinstance(obj, str):
            return obj.replace(old_text, new_text)
        else:
            return obj

    # å¤„ç†æ•´ä¸ª JSON å¯¹è±¡
    processed_data = replace_text(data)

    # å°†å¤„ç†åçš„å¯¹è±¡è½¬å› JSON å­—ç¬¦ä¸²
    return json.dumps(processed_data, ensure_ascii=False)

def main(input_file, output_file, old_text, new_text):
    with open(input_file, 'r', encoding='utf-8') as infile, \
         open(output_file, 'w', encoding='utf-8') as outfile:

        # è®¡ç®—æ€»è¡Œæ•°ç”¨äºè¿›åº¦æ¡
        total_lines = sum(1 for _ in infile)
        infile.seek(0)  # é‡ç½®æ–‡ä»¶æŒ‡é’ˆåˆ°å¼€å¤´

        # ä½¿ç”¨ tqdm åˆ›å»ºè¿›åº¦æ¡
        for line in tqdm(infile, total=total_lines, desc="Processing"):
            processed_line = process_line(line.strip(), old_text, new_text)
            outfile.write(processed_line + '\n')

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Replace text in a JSONL file.")
    parser.add_argument("input_file", help="Input JSONL file to process")
    parser.add_argument("output_file", help="Output file for processed JSONL")
    parser.add_argument("--old_text", default="å°–ç±³", help="Text to be replaced")
    parser.add_argument("--new_text", default="8wekyb3d8bbwe", help="Text to replace with")
    args = parser.parse_args()

    main(args.input_file, args.output_file, args.old_text, args.new_text)
```

æ‰§è¡Œè„šæœ¬

```sh
cd ~/finetune/data
python change_script.py ./assistant_Tuner.jsonl ./assistant_Tuner_change.jsonl
```

![image-20241031201403767](1-6_xtuner.assets/image-20241031201403767.png)

æŸ¥çœ‹æ•°æ®

```sh
cd ~/finetune/data
cat assistant_Tuner_change.jsonl | head -n 3
```

![image-20241031201455811](1-6_xtuner.assets/image-20241031201455811.png)

## è®­ç»ƒ

### å¤åˆ¶æ¨¡å‹

```sh
mkdir /root/finetune/models && cd /root/finetune/models

ln -s /root/share/new_models/Shanghai_AI_Laboratory/internlm2_5-7b-chat /root/finetune/models/internlm2_5-7b-chat
```

![image-20241031201715452](1-6_xtuner.assets/image-20241031201715452.png)

### ä¿®æ”¹ Config

```sh
cd /root/finetune
mkdir config && cd config
xtuner copy-cfg internlm2_5_chat_7b_qlora_alpaca_e3 ./
```

ä¿®æ”¹çš„é…ç½®å¦‚ä¸‹

```python
# Copyright (c) OpenMMLab. All rights reserved.
import torch
from datasets import load_dataset
from mmengine.dataset import DefaultSampler
from mmengine.hooks import (CheckpointHook, DistSamplerSeedHook, IterTimerHook,
                            LoggerHook, ParamSchedulerHook)
from mmengine.visualization import Visualizer, LocalVisBackend, TensorboardVisBackend
from mmengine.optim import AmpOptimWrapper, CosineAnnealingLR, LinearLR
from peft import LoraConfig
from torch.optim import AdamW
from transformers import (AutoModelForCausalLM, AutoTokenizer,
                          BitsAndBytesConfig)

from xtuner.dataset import process_hf_dataset
from xtuner.dataset.collate_fns import default_collate_fn
from xtuner.dataset.map_fns import alpaca_map_fn, template_map_fn_factory
from xtuner.engine.hooks import (DatasetInfoHook, EvaluateChatHook,
                                 VarlenAttnArgsToMessageHubHook)
from xtuner.engine.runner import TrainLoop
from xtuner.model import SupervisedFinetune
from xtuner.parallel.sequence import SequenceParallelSampler
from xtuner.utils import PROMPT_TEMPLATE, SYSTEM_TEMPLATE

#######################################################################
#                          PART 1  Settings                           #
#######################################################################
# Model
pretrained_model_name_or_path = '/root/finetune/models/internlm2_5-7b-chat'
use_varlen_attn = False

# Data
alpaca_en_path = '/root/finetune/data/assistant_Tuner_change.jsonl'
prompt_template = PROMPT_TEMPLATE.internlm2_chat
max_length = 2048
pack_to_max_length = True

# parallel
sequence_parallel_size = 1

# Scheduler & Optimizer
batch_size = 4  # per_device
accumulative_counts = 1
accumulative_counts *= sequence_parallel_size
dataloader_num_workers = 0
max_epochs = 3
optim_type = AdamW
lr = 2e-4
betas = (0.9, 0.999)
weight_decay = 0
max_norm = 1  # grad clip
warmup_ratio = 0.03

# Save
save_steps = 500
save_total_limit = 2  # Maximum checkpoints to keep (-1 means unlimited)

# Evaluate the generation performance during the training
evaluation_freq = 500
SYSTEM = SYSTEM_TEMPLATE.alpaca
evaluation_inputs = [
    'è¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±', 'Please introduce yourself'
]

#######################################################################
#                      PART 2  Model & Tokenizer                      #
#######################################################################
tokenizer = dict(
    type=AutoTokenizer.from_pretrained,
    pretrained_model_name_or_path=pretrained_model_name_or_path,
    trust_remote_code=True,
    padding_side='right')

model = dict(
    type=SupervisedFinetune,
    use_varlen_attn=use_varlen_attn,
    llm=dict(
        type=AutoModelForCausalLM.from_pretrained,
        pretrained_model_name_or_path=pretrained_model_name_or_path,
        trust_remote_code=True,
        torch_dtype=torch.float16,
        quantization_config=dict(
            type=BitsAndBytesConfig,
            load_in_4bit=True,
            load_in_8bit=False,
            llm_int8_threshold=6.0,
            llm_int8_has_fp16_weight=False,
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type='nf4')),
    lora=dict(
        type=LoraConfig,
        r=64,
        lora_alpha=16,
        lora_dropout=0.1,
        bias='none',
        task_type='CAUSAL_LM'))

#######################################################################
#                      PART 3  Dataset & Dataloader                   #
#######################################################################
alpaca_en = dict(
    type=process_hf_dataset,
    dataset=dict(type=load_dataset, path='json', data_files=dict(train=alpaca_en_path)),
    tokenizer=tokenizer,
    max_length=max_length,
    dataset_map_fn=None,
    template_map_fn=dict(
        type=template_map_fn_factory, template=prompt_template),
    remove_unused_columns=True,
    shuffle_before_pack=True,
    pack_to_max_length=pack_to_max_length,
    use_varlen_attn=use_varlen_attn)

sampler = SequenceParallelSampler \
    if sequence_parallel_size > 1 else DefaultSampler
train_dataloader = dict(
    batch_size=batch_size,
    num_workers=dataloader_num_workers,
    dataset=alpaca_en,
    sampler=dict(type=sampler, shuffle=True),
    collate_fn=dict(type=default_collate_fn, use_varlen_attn=use_varlen_attn))

#######################################################################
#                    PART 4  Scheduler & Optimizer                    #
#######################################################################
# optimizer
optim_wrapper = dict(
    type=AmpOptimWrapper,
    optimizer=dict(
        type=optim_type, lr=lr, betas=betas, weight_decay=weight_decay),
    clip_grad=dict(max_norm=max_norm, error_if_nonfinite=False),
    accumulative_counts=accumulative_counts,
    loss_scale='dynamic',
    dtype='float16')

# learning policy
# More information: https://github.com/open-mmlab/mmengine/blob/main/docs/en/tutorials/param_scheduler.md  # noqa: E501
param_scheduler = [
    dict(
        type=LinearLR,
        start_factor=1e-5,
        by_epoch=True,
        begin=0,
        end=warmup_ratio * max_epochs,
        convert_to_iter_based=True),
    dict(
        type=CosineAnnealingLR,
        eta_min=0.0,
        by_epoch=True,
        begin=warmup_ratio * max_epochs,
        end=max_epochs,
        convert_to_iter_based=True)
]

# train, val, test setting
train_cfg = dict(type=TrainLoop, max_epochs=max_epochs)

#######################################################################
#                           PART 5  Runtime                           #
#######################################################################
# Log the dialogue periodically during the training process, optional
custom_hooks = [
    dict(type=DatasetInfoHook, tokenizer=tokenizer),
    dict(
        type=EvaluateChatHook,
        tokenizer=tokenizer,
        every_n_iters=evaluation_freq,
        evaluation_inputs=evaluation_inputs,
        system=SYSTEM,
        prompt_template=prompt_template)
]

if use_varlen_attn:
    custom_hooks += [dict(type=VarlenAttnArgsToMessageHubHook)]

# configure default hooks
default_hooks = dict(
    # record the time of every iteration.
    timer=dict(type=IterTimerHook),
    # print log every 10 iterations.
    logger=dict(type=LoggerHook, log_metric_by_epoch=False, interval=10),
    # enable the parameter scheduler.
    param_scheduler=dict(type=ParamSchedulerHook),
    # save checkpoint per `save_steps`.
    checkpoint=dict(
        type=CheckpointHook,
        by_epoch=False,
        interval=save_steps,
        max_keep_ckpts=save_total_limit),
    # set sampler seed in distributed evrionment.
    sampler_seed=dict(type=DistSamplerSeedHook),
)

# configure environment
env_cfg = dict(
    # whether to enable cudnn benchmark
    cudnn_benchmark=False,
    # set multi process parameters
    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0),
    # set distributed parameters
    dist_cfg=dict(backend='nccl'),
)

# set visualizer
visualizer = dict(
    type=Visualizer,
    vis_backends=[dict(type=LocalVisBackend), dict(type=TensorboardVisBackend)],
)

# set log level
log_level = 'INFO'

# load from which checkpoint
load_from = None

# whether to resume training from the loaded checkpoint
resume = False

# Defaults to use random seed and disable `deterministic`
randomness = dict(seed=None, deterministic=False)

# set log processor
log_processor = dict(by_epoch=False)
```

### å¯åŠ¨å¾®è°ƒ

`xtuner train` å‘½ä»¤ç”¨äºå¯åŠ¨æ¨¡å‹å¾®è°ƒè¿›ç¨‹ã€‚è¯¥å‘½ä»¤éœ€è¦ä¸€ä¸ªå‚æ•°ï¼š`CONFIG` ç”¨äºæŒ‡å®šå¾®è°ƒé…ç½®æ–‡ä»¶ã€‚è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨ä¿®æ”¹å¥½çš„é…ç½®æ–‡ä»¶ `internlm2_5_chat_7b_qlora_alpaca_e3_copy.py`ã€‚
 è®­ç»ƒè¿‡ç¨‹ä¸­äº§ç”Ÿçš„æ‰€æœ‰æ–‡ä»¶ï¼ŒåŒ…æ‹¬æ—¥å¿—ã€é…ç½®æ–‡ä»¶ã€æ£€æŸ¥ç‚¹æ–‡ä»¶ã€å¾®è°ƒåçš„æ¨¡å‹ç­‰ï¼Œé»˜è®¤ä¿å­˜åœ¨ `work_dirs` ç›®å½•ä¸‹ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥é€šè¿‡æ·»åŠ  `--work-dir` æŒ‡å®šç‰¹å®šçš„æ–‡ä»¶ä¿å­˜ä½ç½®ã€‚`--deepspeed` åˆ™ä¸ºä½¿ç”¨ deepspeedï¼Œ deepspeed å¯ä»¥èŠ‚çº¦æ˜¾å­˜ã€‚

```sh
cd /root/finetune

xtuner train ./config/internlm2_5_chat_7b_qlora_alpaca_e3_copy.py --deepspeed deepspeed_zero2 --work-dir ./work_dirs/assistTuner
```

![image-20241031204445314](1-6_xtuner.assets/image-20241031204445314.png)

### æƒé‡è½¬æ¢

æ¨¡å‹è½¬æ¢çš„æœ¬è´¨å…¶å®å°±æ˜¯å°†åŸæœ¬ä½¿ç”¨ Pytorch è®­ç»ƒå‡ºæ¥çš„æ¨¡å‹æƒé‡æ–‡ä»¶è½¬æ¢ä¸ºç›®å‰é€šç”¨çš„ HuggingFace æ ¼å¼æ–‡ä»¶ï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯ä»¥é€šè¿‡ä»¥ä¸‹å‘½ä»¤æ¥å®ç°ä¸€é”®è½¬æ¢ã€‚

æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ `xtuner convert pth_to_hf` å‘½ä»¤æ¥è¿›è¡Œæ¨¡å‹æ ¼å¼è½¬æ¢ã€‚

> `xtuner convert pth_to_hf` å‘½ä»¤ç”¨äºè¿›è¡Œæ¨¡å‹æ ¼å¼è½¬æ¢ã€‚è¯¥å‘½ä»¤éœ€è¦ä¸‰ä¸ªå‚æ•°ï¼š`CONFIG` è¡¨ç¤ºå¾®è°ƒçš„é…ç½®æ–‡ä»¶ï¼Œ `PATH_TO_PTH_MODEL` è¡¨ç¤ºå¾®è°ƒçš„æ¨¡å‹æƒé‡æ–‡ä»¶è·¯å¾„ï¼Œå³è¦è½¬æ¢çš„æ¨¡å‹æƒé‡ï¼Œ `SAVE_PATH_TO_HF_MODEL` è¡¨ç¤ºè½¬æ¢åçš„ HuggingFace æ ¼å¼æ–‡ä»¶çš„ä¿å­˜è·¯å¾„ã€‚

é™¤æ­¤ä¹‹å¤–ï¼Œæˆ‘ä»¬å…¶å®è¿˜å¯ä»¥åœ¨è½¬æ¢çš„å‘½ä»¤ä¸­æ·»åŠ å‡ ä¸ªé¢å¤–çš„å‚æ•°ï¼ŒåŒ…æ‹¬ï¼š

| å‚æ•°å                | è§£é‡Š                                         |
| --------------------- | -------------------------------------------- |
| --fp32                | ä»£è¡¨ä»¥fp32çš„ç²¾åº¦å¼€å¯ï¼Œå‡å¦‚ä¸è¾“å…¥åˆ™é»˜è®¤ä¸ºfp16 |
| --max-shard-size {GB} | ä»£è¡¨æ¯ä¸ªæƒé‡æ–‡ä»¶æœ€å¤§çš„å¤§å°ï¼ˆé»˜è®¤ä¸º2GBï¼‰      |

```sh
cd /root/finetune/work_dirs/assistTuner

export MKL_SERVICE_FORCE_INTEL=1
export MKL_THREADING_LAYER=GNU
xtuner convert pth_to_hf ./internlm2_5_chat_7b_qlora_alpaca_e3_copy.py ./iter_462.pth ./hf
```

è½¬æ¢å®Œæˆåï¼Œå¯ä»¥çœ‹åˆ°æ¨¡å‹è¢«è½¬æ¢ä¸º HuggingFace ä¸­å¸¸ç”¨çš„ .bin æ ¼å¼æ–‡ä»¶ï¼Œè¿™å°±ä»£è¡¨ç€æ–‡ä»¶æˆåŠŸè¢«è½¬åŒ–ä¸º HuggingFace æ ¼å¼äº†ã€‚

æ­¤æ—¶ï¼Œhf æ–‡ä»¶å¤¹å³ä¸ºæˆ‘ä»¬å¹³æ—¶æ‰€ç†è§£çš„æ‰€è°“ â€œLoRA æ¨¡å‹æ–‡ä»¶â€

> å¯ä»¥ç®€å•ç†è§£ï¼šLoRA æ¨¡å‹æ–‡ä»¶ = Adapter

![image-20241104130255141](1-6_xtuner.assets/image-20241104130255141.png)

### æ¨¡å‹åˆå¹¶

å¯¹äº LoRA æˆ–è€… QLoRA å¾®è°ƒå‡ºæ¥çš„æ¨¡å‹å…¶å®å¹¶ä¸æ˜¯ä¸€ä¸ªå®Œæ•´çš„æ¨¡å‹ï¼Œè€Œæ˜¯ä¸€ä¸ªé¢å¤–çš„å±‚ï¼ˆAdapterï¼‰ï¼Œè®­ç»ƒå®Œçš„è¿™ä¸ªå±‚æœ€ç»ˆè¿˜æ˜¯è¦ä¸åŸæ¨¡å‹è¿›è¡Œåˆå¹¶æ‰èƒ½è¢«æ­£å¸¸çš„ä½¿ç”¨ã€‚

> å¯¹äºå…¨é‡å¾®è°ƒçš„æ¨¡å‹ï¼ˆfullï¼‰å…¶å®æ˜¯ä¸éœ€è¦è¿›è¡Œæ•´åˆè¿™ä¸€æ­¥çš„ï¼Œå› ä¸ºå…¨é‡å¾®è°ƒä¿®æ”¹çš„æ˜¯åŸæ¨¡å‹çš„æƒé‡è€Œéå¾®è°ƒä¸€ä¸ªæ–°çš„ Adapter ï¼Œå› æ­¤æ˜¯ä¸éœ€è¦è¿›è¡Œæ¨¡å‹æ•´åˆçš„ã€‚

åœ¨ XTuner ä¸­æä¾›äº†ä¸€é”®åˆå¹¶çš„å‘½ä»¤ `xtuner convert merge`ï¼Œåœ¨ä½¿ç”¨å‰æˆ‘ä»¬éœ€è¦å‡†å¤‡å¥½ä¸‰ä¸ªè·¯å¾„ï¼ŒåŒ…æ‹¬åŸæ¨¡å‹çš„è·¯å¾„ã€è®­ç»ƒå¥½çš„ Adapter å±‚çš„ï¼ˆæ¨¡å‹æ ¼å¼è½¬æ¢åçš„ï¼‰è·¯å¾„ä»¥åŠæœ€ç»ˆä¿å­˜çš„è·¯å¾„ã€‚

> `xtuner convert merge`å‘½ä»¤ç”¨äºåˆå¹¶æ¨¡å‹ã€‚è¯¥å‘½ä»¤éœ€è¦ä¸‰ä¸ªå‚æ•°ï¼š`LLM` è¡¨ç¤ºåŸæ¨¡å‹è·¯å¾„ï¼Œ`ADAPTER` è¡¨ç¤º Adapter å±‚çš„è·¯å¾„ï¼Œ `SAVE_PATH` è¡¨ç¤ºåˆå¹¶åçš„æ¨¡å‹æœ€ç»ˆçš„ä¿å­˜è·¯å¾„ã€‚

åœ¨æ¨¡å‹åˆå¹¶è¿™ä¸€æ­¥è¿˜æœ‰å…¶ä»–å¾ˆå¤šçš„å¯é€‰å‚æ•°ï¼ŒåŒ…æ‹¬ï¼š

| å‚æ•°å                 | è§£é‡Š                                                         |
| ---------------------- | ------------------------------------------------------------ |
| --max-shard-size {GB}  | ä»£è¡¨æ¯ä¸ªæƒé‡æ–‡ä»¶æœ€å¤§çš„å¤§å°ï¼ˆé»˜è®¤ä¸º2GBï¼‰                      |
| --device {device_name} | è¿™é‡ŒæŒ‡çš„å°±æ˜¯deviceçš„åç§°ï¼Œå¯é€‰æ‹©çš„æœ‰cudaã€cpuå’Œautoï¼Œé»˜è®¤ä¸ºcudaå³ä½¿ç”¨gpuè¿›è¡Œè¿ç®— |
| --is-clip              | è¿™ä¸ªå‚æ•°ä¸»è¦ç”¨äºç¡®å®šæ¨¡å‹æ˜¯ä¸æ˜¯CLIPæ¨¡å‹ï¼Œå‡å¦‚æ˜¯çš„è¯å°±è¦åŠ ä¸Šï¼Œä¸æ˜¯å°±ä¸éœ€è¦æ·»åŠ  |

```sh
cd /root/finetune/work_dirs/assistTuner

export MKL_SERVICE_FORCE_INTEL=1
export MKL_THREADING_LAYER=GNU
xtuner convert merge /root/finetune/models/internlm2_5-7b-chat ./hf ./merged --max-shard-size 2GB
```

![image-20241104130701840](1-6_xtuner.assets/image-20241104130701840.png)

## æ¨¡å‹ WebUI å¯¹è¯

```sh
cd ~/Tutorial/tools/L1_XTuner_code
```

```sh
# ç›´æ¥ä¿®æ”¹è„šæœ¬æ–‡ä»¶ç¬¬18è¡Œ
- model_name_or_path = "Shanghai_AI_Laboratory/internlm2_5-7b-chat"
+ model_name_or_path = "/root/finetune/work_dirs/assistTuner/merged"
```

![image-20241104130811880](1-6_xtuner.assets/image-20241104130811880.png)

ä¹‹åå¯åŠ¨åº”ç”¨

```sh
streamlit run /root/Tutorial/tools/L1_XTuner_code/xtuner_streamlit_demo.py
```

æ˜ å°„ç«¯å£

```sh
ssh root@ssh.intern-ai.org.cn -p 43681 -CNg -L 8501:127.0.0.1:8501 -o StrictHostKeyChecking=no UserKnownHostsFile=/dev/null
```

![image-20241104130915821](1-6_xtuner.assets/image-20241104130915821.png)

æµè§ˆå™¨è®¿é—®ï¼šhttp://127.0.0.1:8501 æ¥è¿›è¡Œå¯¹è¯

![image-20241104134242630](1-6_xtuner.assets/image-20241104134242630.png)

# å°†æ¨¡å‹ä¸Šä¼ åˆ° openxlab

## åˆ›å»ºä»“åº“

![image-20241104135049962](1-6_xtuner.assets/image-20241104135049962.png)

## ä¸Šä¼ æ–‡ä»¶

![image-20241104142245913](1-6_xtuner.assets/image-20241104142245913.png)

## æŸ¥çœ‹ç½‘é¡µ

https://openxlab.org.cn/models/detail/NagatoYuki0943/internlm2_5-7b-chat-self-assistant

![image-20241104145051065](1-6_xtuner.assets/image-20241104145051065.png)

# éƒ¨ç½²åˆ° openxlab

## é¦–å…ˆåˆ›å»ºä»“åº“

https://github.com/NagatoYuki0943/XTuner-Web-Demo

## ç¼–å†™ä»£ç ï¼Œä½¿ç”¨ lmdeploy è¿›è¡ŒåŠ é€Ÿéƒ¨ç½²

```python
import os
import gradio as gr
from infer_engine import InferEngine, LmdeployConfig
from typing import Generator, Sequence, Any
import threading
from loguru import logger


logger.info(f"gradio version: {gr.__version__}")


# clone æ¨¡å‹
MODEL_PATH = 'internlm2_5-7b-chat-self-assistant'
os.system(f'git clone https://code.openxlab.org.cn/NagatoYuki0943/internlm2_5-7b-chat-self-assistant.git {MODEL_PATH}')
os.system(f'cd {MODEL_PATH} && git lfs pull')

SYSTEM_PROMPT = """"""

LMDEPLOY_CONFIG = LmdeployConfig(
    model_path=MODEL_PATH,
    backend="turbomind",
    model_name="internlm2",
    model_format="hf",
    tp=1,  # Tensor Parallelism.
    max_batch_size=128,
    cache_max_entry_count=0.8,  # è°ƒæ•´ KV Cache çš„å ç”¨æ¯”ä¾‹ä¸º0.8
    quant_policy=0,  # KV Cache é‡åŒ–, 0 ä»£è¡¨ç¦ç”¨, 4 ä»£è¡¨ 4bit é‡åŒ–, 8 ä»£è¡¨ 8bit é‡åŒ–
    system_prompt=SYSTEM_PROMPT,
    deploy_method="local",
)

# è½½å…¥æ¨¡å‹
infer_engine = InferEngine(
    backend="lmdeploy",  # transformers, lmdeploy, api
    lmdeploy_config=LMDEPLOY_CONFIG,
)


class InterFace:
    global_session_id: int = 0
    lock = threading.Lock()


enable_btn = gr.update(interactive=True)
disable_btn = gr.update(interactive=False)
btn = dict[str, Any]


def chat_stream(
    query: str,
    history: Sequence
    | None = None,  # [['What is the capital of France?', 'The capital of France is Paris.'], ['Thanks', 'You are Welcome']]
    max_new_tokens: int = 1024,
    temperature: float = 0.8,
    top_p: float = 0.8,
    top_k: int = 40,
    state_session_id: int | None = None,
) -> Generator[tuple[Sequence, btn, btn, btn, btn], None, None]:
    history = [] if history is None else list(history)

    logger.info(f"{state_session_id = }")
    logger.info(
        {
            "max_new_tokens": max_new_tokens,
            "temperature": temperature,
            "top_p": top_p,
            "top_k": top_k,
        }
    )

    query = query.strip()
    if query is None or len(query) < 1:
        logger.warning("query is None, return history")
        yield history, enable_btn, enable_btn, enable_btn, enable_btn
        return
    logger.info(f"query: {query}")
    logger.info(f"history before: {history}")

    yield history + [[query, None]], disable_btn, disable_btn, disable_btn, disable_btn

    responses = []
    for response in infer_engine.chat_stream(
        query=query,
        history=history,
        max_new_tokens=max_new_tokens,
        temperature=temperature,
        top_p=top_p,
        top_k=top_k,
        session_id=state_session_id,
    ):
        responses.append(response)
        yield (
            history + [[query, "".join(responses)]],
            disable_btn,
            disable_btn,
            disable_btn,
            disable_btn,
        )

    _response = "".join(responses)
    yield history + [[query, _response]], enable_btn, enable_btn, enable_btn, enable_btn

    logger.info(f"history after: {history + [[query, _response]]}")


def regenerate(
    history: Sequence
    | None = None,  # [['What is the capital of France?', 'The capital of France is Paris.'], ['Thanks', 'You are Welcome']]
    max_new_tokens: int = 1024,
    temperature: float = 0.8,
    top_p: float = 0.8,
    top_k: int = 40,
    state_session_id: int | None = None,
) -> Generator[tuple[Sequence, btn, btn, btn, btn], None, None]:
    history = [] if history is None else list(history)

    # é‡æ–°ç”Ÿæˆæ—¶è¦æŠŠæœ€åçš„queryå’Œresponseå¼¹å‡º,é‡ç”¨query
    if len(history) > 0:
        query, _ = history.pop(-1)
        yield from chat_stream(
            query=query,
            history=history,
            max_new_tokens=max_new_tokens,
            temperature=temperature,
            top_p=top_p,
            top_k=top_k,
            state_session_id=state_session_id,
        )
    else:
        logger.warning("no history, can't regenerate")
        yield history, enable_btn, enable_btn, enable_btn, enable_btn


def revocery(history: Sequence | None = None) -> tuple[str, Sequence]:
    """æ¢å¤åˆ°ä¸Šä¸€è½®å¯¹è¯"""
    history = [] if history is None else list(history)
    query = ""
    if len(history) > 0:
        query, _ = history.pop(-1)
    return query, history


def main():
    block = gr.Blocks()
    with block as demo:
        state_session_id = gr.State(0)

        with gr.Row(equal_height=True):
            with gr.Column(scale=15):
                gr.Markdown("""<h1><center>InternLM 2.5-7b Chatbot</center></h1>
                    <center>InternLM Self Assistant</center>
                    """)
            # gr.Image(value=LOGO_PATH, scale=1, min_width=10,show_label=False, show_download_button=False)

        with gr.Row():
            with gr.Column(scale=4):
                # åˆ›å»ºèŠå¤©æ¡†
                chatbot = gr.Chatbot(
                    height=500,
                    show_copy_button=True,
                    placeholder="å†…å®¹ç”± AI å¤§æ¨¡å‹ç”Ÿæˆï¼Œè¯·ä»”ç»†ç”„åˆ«ã€‚",
                )

                # ç»„å†…çš„ç»„ä»¶æ²¡æœ‰é—´è·
                with gr.Group():
                    with gr.Row():
                        # åˆ›å»ºä¸€ä¸ªæ–‡æœ¬æ¡†ç»„ä»¶ï¼Œç”¨äºè¾“å…¥ promptã€‚
                        query = gr.Textbox(
                            lines=1,
                            label="Prompt / é—®é¢˜",
                            placeholder="Enter å‘é€; Shift + Enter æ¢è¡Œ / Enter to send; Shift + Enter to wrap",
                        )
                        # åˆ›å»ºæäº¤æŒ‰é’®ã€‚
                        # variant https://www.gradio.app/docs/button
                        # scale https://www.gradio.app/guides/controlling-layout
                        submit = gr.Button("ğŸ’¬ Chat", variant="primary", scale=0)

                with gr.Row():
                    # åˆ›å»ºä¸€ä¸ªé‡æ–°ç”ŸæˆæŒ‰é’®ï¼Œç”¨äºé‡æ–°ç”Ÿæˆå½“å‰å¯¹è¯å†…å®¹ã€‚
                    regen = gr.Button("ğŸ”„ Retry", variant="secondary")
                    undo = gr.Button("â†©ï¸ Undo", variant="secondary")
                    # åˆ›å»ºä¸€ä¸ªæ¸…é™¤æŒ‰é’®ï¼Œç”¨äºæ¸…é™¤èŠå¤©æœºå™¨äººç»„ä»¶çš„å†…å®¹ã€‚
                    clear = gr.ClearButton(
                        components=[chatbot], value="ğŸ—‘ï¸ Clear", variant="stop"
                    )

                # æŠ˜å 
                with gr.Accordion("Advanced Options", open=False):
                    with gr.Row():
                        max_new_tokens = gr.Slider(
                            minimum=1,
                            maximum=2048,
                            value=1024,
                            step=1,
                            label="Max new tokens",
                        )
                        temperature = gr.Slider(
                            minimum=0.01,
                            maximum=2,
                            value=0.8,
                            step=0.01,
                            label="Temperature",
                        )
                        top_p = gr.Slider(
                            minimum=0.01, maximum=1, value=0.8, step=0.01, label="Top_p"
                        )
                        top_k = gr.Slider(
                            minimum=1, maximum=100, value=40, step=1, label="Top_k"
                        )

                gr.Examples(
                    examples=[
                        ["ä½ æ˜¯è°"],
                        ["ä½ å¯ä»¥å¸®æˆ‘åšä»€ä¹ˆ"],
                    ],
                    inputs=[query],
                    label="ç¤ºä¾‹é—®é¢˜ / Example questions",
                )

            # å›è½¦æäº¤(æ— æ³•ç¦æ­¢æŒ‰é’®)
            query.submit(
                chat_stream,
                inputs=[
                    query,
                    chatbot,
                    max_new_tokens,
                    temperature,
                    top_p,
                    top_k,
                    state_session_id,
                ],
                outputs=[chatbot, submit, regen, undo, clear],
            )

            # æ¸…ç©ºquery
            query.submit(
                lambda: gr.Textbox(value=""),
                inputs=[],
                outputs=[query],
            )

            # æŒ‰é’®æäº¤
            submit.click(
                chat_stream,
                inputs=[
                    query,
                    chatbot,
                    max_new_tokens,
                    temperature,
                    top_p,
                    top_k,
                    state_session_id,
                ],
                outputs=[chatbot, submit, regen, undo, clear],
            )

            # æ¸…ç©ºquery
            submit.click(
                lambda: gr.Textbox(value=""),
                inputs=[],
                outputs=[query],
            )

            # é‡æ–°ç”Ÿæˆ
            regen.click(
                regenerate,
                inputs=[
                    chatbot,
                    max_new_tokens,
                    temperature,
                    top_p,
                    top_k,
                    state_session_id,
                ],
                outputs=[chatbot, submit, regen, undo, clear],
            )

            # æ’¤é”€
            undo.click(revocery, inputs=[chatbot], outputs=[query, chatbot])

        gr.Markdown("""æé†’ï¼š<br>
        1. å†…å®¹ç”± AI å¤§æ¨¡å‹ç”Ÿæˆï¼Œè¯·ä»”ç»†ç”„åˆ«ã€‚<br>
        """)

        # åˆå§‹åŒ–session_id
        def init():
            with InterFace.lock:
                InterFace.global_session_id += 1
            new_session_id = InterFace.global_session_id
            return new_session_id

        demo.load(init, inputs=None, outputs=[state_session_id])

    # threads to consume the request
    gr.close_all()

    # è®¾ç½®é˜Ÿåˆ—å¯åŠ¨
    demo.queue(
        max_size=None,  # If None, the queue size will be unlimited.
        default_concurrency_limit=100,  # æœ€å¤§å¹¶å‘é™åˆ¶
    )

    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=True,
        max_threads=100,
    )


if __name__ == "__main__":
    main()
```

## åˆ›å»ºåº”ç”¨

![image-20241105071429960](1-6_xtuner.assets/image-20241105071429960.png)

## å¯åŠ¨

![image-20241105071522780](1-6_xtuner.assets/image-20241105071522780.png)

## è®¿é—®

https://openxlab.org.cn/apps/detail/NagatoYuki0943/XTuner-self-assistant

