# Âü∫‰∫é LlamaIndex ÊûÑÂª∫Ëá™Â∑±ÁöÑ RAG Áü•ËØÜÂ∫ì

1. ÂÆâË£Ö‰æùËµñ

```s
pip install sentence-transformers BCEmbedding llama-index llama-index-llms-huggingface llama-index-embeddings-huggingface llama-index-embeddings-instructor
```

![image-20240806124802512](1-4_rag.assets/image-20240806124802512.png)

2. ÂáÜÂ§áÊ®°Âûã

```sh
cd ~
mkdir llamaindex_demo
cd llamaindex_demo
mkdir model
cd model
ln -s /root/share/new_models/Shanghai_AI_Laboratory/internlm2_5-1_8b-chat/ ./
ln -s /root/share/new_models/maidalun1020/bce-embedding-base_v1/ ./
ls
```

![image-20240806125126768](1-4_rag.assets/image-20240806125126768.png)

3. ‰∏ãËΩΩ nltk Áõ∏ÂÖ≥ËµÑÊ∫ê

Áî®‰ª•‰∏ãÂëΩ‰ª§‰∏ãËΩΩ nltk ËµÑÊ∫êÂπ∂Ëß£ÂéãÂà∞ÊúçÂä°Âô®‰∏äÔºö

```
cd /root
git clone https://gitee.com/yzy0612/nltk_data.git  --branch gh-pages
cd nltk_data
mv packages/*  ./
cd tokenizers
unzip punkt.zip
cd ../taggers
unzip averaged_perceptron_tagger.zip
```

4. ÂàõÂª∫Êñá‰ª∂ÊâßË°åÊ®°ÂûãÈóÆÁ≠î

```sh
cd ~/llamaindex_demo
touch llamaindex_internlm.py
```

```python
from llama_index.llms.huggingface import HuggingFaceLLM
from llama_index.core.llms import ChatMessage


llm = HuggingFaceLLM(
    model_name = "./model/internlm2_5-1_8b-chat",
    tokenizer_name = "./model/internlm2_5-1_8b-chat",
    model_kwargs = {"trust_remote_code": True},
    tokenizer_kwargs = {"trust_remote_code": True},
    device_map = "auto",
    max_new_tokens = 256,
    generate_kwargs = {"do_sample": True, "temperature": 0.1, "top_k": 50, "top_p": 0.8, "eos_token_id": 2},
)

response = llm.chat(messages=[ChatMessage(content="msi x670 godlike ÊúâÂá†‰∏™ÂÜÖÂ≠òÊèíÊßΩÔºüÊîØÊåÅÁöÑÈ¢ëÁéáÊòØÂ§öÂ∞ëÔºü")])
print(response)
```

> ÂõûÁ≠îÁöÑ‰∏çÊ≠£Á°ÆÔºåÂ∞Ü MSI x670 ËØÜÂà´‰∏∫‰∫ÜÊòæÂç°ÔºåÂÆûÈôÖ‰∏äÊó∂‰∏ªÊùø

```sh
(lmdeploy) root@intern-studio-030876:~/llamaindex_demo# python llamaindex_internlm.py 
/root/.conda/envs/lmdeploy/lib/python3.10/site-packages/pydantic/_internal/_fields.py:161: UserWarning: Field "model_id" has conflict with protected namespace "model_".

You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.
  warnings.warn(
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:15<00:00,  7.82s/it]
assistant: MSI x670 Á≥ªÂàóÊòæÂç°ÊòØMSIÊòæÂç°ÂìÅÁâå‰∏≠ÊØîËæÉÈ´òÁ´ØÁöÑÊòæÂç°ÔºåÊã•ÊúâÂº∫Â§ßÁöÑÊÄßËÉΩÂíåÊï£ÁÉ≠ÊÄßËÉΩÔºåÊòØ‰∏ÄÊ¨æÈùûÂ∏∏ÂèóÊ¨¢ËøéÁöÑÊòæÂç°„ÄÇ‰ª•‰∏ãÊòØÂÖ≥‰∫éMSI x670ÊòæÂç°ÁöÑ‰∏Ä‰∫õÂü∫Êú¨‰ø°ÊÅØÔºö

1. **ÂÜÖÂ≠òÊèíÊßΩÊï∞Èáè**Ôºö
   - MSI x670ÊòæÂç°ÈÄöÂ∏∏ÊîØÊåÅ2‰∏™ÂÜÖÂ≠òÊèíÊßΩÔºåÊØè‰∏™ÊèíÊßΩÂèØ‰ª•ÂÆâË£Ö64GBÊàñ128GBÁöÑDDR4ÂÜÖÂ≠ò„ÄÇ

2. **ÊîØÊåÅÁöÑÈ¢ëÁéá**Ôºö
   - ÂÜÖÂ≠òÈ¢ëÁéáÈÄöÂ∏∏ÊîØÊåÅDDR4 2133MHzÂà∞2833MHzÔºåÂÖ∑‰ΩìÈ¢ëÁéáÂèØËÉΩÂõ†ÊòæÂç°ÂûãÂè∑Âíå‰∏ªÊùøËäØÁâáÁªÑ‰∏çÂêåËÄåÊúâÊâÄÂ∑ÆÂºÇ„ÄÇ

3. **ÂÖ∂‰ªñÁâπÊÄß**Ôºö
   - ÊîØÊåÅPCIe 16xÊàñPCIe 32xÊèíÊßΩÔºåÈÄÇÂêàÈúÄË¶ÅÊâ©Â±ïÂÜÖÂ≠òÂ∏¶ÂÆΩÁöÑÈúÄÊ±Ç„ÄÇ
   - ÊîØÊåÅPCIe 3.0ÊèíÊßΩÔºåÊèê‰æõÊõ¥È´òÁöÑÂ∏¶ÂÆΩÂíåÊõ¥‰ΩéÁöÑÂª∂Ëøü„ÄÇ
   - ÊîØÊåÅPCIe 4.0ÊèíÊßΩÔºåËøõ‰∏ÄÊ≠•ÊèêÂçáÂ∏¶ÂÆΩÂíåÊÄßËÉΩ„ÄÇ

ËØ∑Ê≥®ÊÑèÔºåÊòæÂç°ÁöÑÂÆûÈôÖÊÄßËÉΩÂíåÂÖºÂÆπÊÄßÂèØËÉΩ‰ºöÂõ†‰∏ªÊùøËäØÁâáÁªÑ„ÄÅ‰∏ªÊùøÂûãÂè∑‰ª•ÂèäÂÖ∑‰ΩìÈÖçÁΩÆÁ≠âÂõ†Á¥†ËÄåÊúâÊâÄ‰∏çÂêå„ÄÇÂú®Ë¥≠‰π∞ÂâçÔºåÂª∫ËÆÆÊü•ÁúãÊòæÂç°ÁöÑÁî®Êà∑ÊâãÂÜåÊàñËÅîÁ≥ªMSIÂÆòÊñπÂÆ¢ÊúçËé∑ÂèñÊúÄÂáÜÁ°ÆÁöÑ‰ø°ÊÅØ„ÄÇ
```

![image-20240806133339354](1-4_rag.assets/image-20240806133339354.png)

5. Ê∑ªÂä† rag Êï∞ÊçÆ

```
cd ~/llamaindex_demo
mkdir data
cd data
```

Â∞ÜÊï∞ÊçÆÊîæÂÖ• data Êñá‰ª∂Â§πÔºå‰∏ãÂõæÂèØ‰ª•ÁúãÂà∞Êàë‰ª¨Ê∑ªÂä†‰∫Ü‰∏Ä‰∏™pdfÊñáÊ°£

![image-20240806131125768](1-4_rag.assets/image-20240806131125768.png)

6. ÂàõÂª∫ rag ‰ª£Á†Å

```sh
cd ~/llamaindex_demo
touch llamaindex_RAG.py
```

```python
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.llms.huggingface import HuggingFaceLLM


# ÂàùÂßãÂåñ‰∏Ä‰∏™HuggingFaceEmbeddingÂØπË±°ÔºåÁî®‰∫éÂ∞ÜÊñáÊú¨ËΩ¨Êç¢‰∏∫ÂêëÈáèË°®Á§∫
embed_model = HuggingFaceEmbedding(
    model_name="./model/bce-embedding-base_v1"
)
# Â∞ÜÂàõÂª∫ÁöÑÂµåÂÖ•Ê®°ÂûãËµãÂÄºÁªôÂÖ®Â±ÄËÆæÁΩÆÁöÑembed_modelÂ±ûÊÄßÔºå
# ËøôÊ†∑Âú®ÂêéÁª≠ÁöÑÁ¥¢ÂºïÊûÑÂª∫ËøáÁ®ã‰∏≠Â∞±‰ºö‰ΩøÁî®Ëøô‰∏™Ê®°Âûã„ÄÇ
Settings.embed_model = embed_model

llm = HuggingFaceLLM(
    model_name = "./model/internlm2_5-1_8b-chat",
    tokenizer_name = "./model/internlm2_5-1_8b-chat",
    model_kwargs = {"trust_remote_code": True},
    tokenizer_kwargs = {"trust_remote_code": True},
    device_map = "auto",
    max_new_tokens = 256,
    generate_kwargs = {"do_sample": True, "temperature": 0.1, "top_k": 50, "top_p": 0.8, "eos_token_id": 2},
)
# ËÆæÁΩÆÂÖ®Â±ÄÁöÑllmÂ±ûÊÄßÔºåËøôÊ†∑Âú®Á¥¢ÂºïÊü•ËØ¢Êó∂‰ºö‰ΩøÁî®Ëøô‰∏™Ê®°Âûã„ÄÇ
Settings.llm = llm

# ‰ªéÊåáÂÆöÁõÆÂΩïËØªÂèñÊâÄÊúâÊñáÊ°£ÔºåÂπ∂Âä†ËΩΩÊï∞ÊçÆÂà∞ÂÜÖÂ≠ò‰∏≠
documents = SimpleDirectoryReader("./data").load_data()
print(f"documents number = {len(documents)}")

# ÂàõÂª∫‰∏Ä‰∏™VectorStoreIndexÔºåÂπ∂‰ΩøÁî®‰πãÂâçÂä†ËΩΩÁöÑÊñáÊ°£Êù•ÊûÑÂª∫Á¥¢Âºï„ÄÇ
# Ê≠§Á¥¢ÂºïÂ∞ÜÊñáÊ°£ËΩ¨Êç¢‰∏∫ÂêëÈáèÔºåÂπ∂Â≠òÂÇ®Ëøô‰∫õÂêëÈáè‰ª•‰æø‰∫éÂø´ÈÄüÊ£ÄÁ¥¢„ÄÇ
index = VectorStoreIndex.from_documents(
    documents = documents,
    embed_model = embed_model
)

# ÂàõÂª∫‰∏Ä‰∏™Êü•ËØ¢ÂºïÊìéÔºåËøô‰∏™ÂºïÊìéÂèØ‰ª•Êé•Êî∂Êü•ËØ¢Âπ∂ËøîÂõûÁõ∏ÂÖ≥ÊñáÊ°£ÁöÑÂìçÂ∫î„ÄÇ
query_engine = index.as_query_engine(llm = llm)

response = query_engine.query("msi x670 godlike ÊúâÂá†‰∏™ÂÜÖÂ≠òÊèíÊßΩÔºüÊîØÊåÅÁöÑÈ¢ëÁéáÊòØÂ§öÂ∞ëÔºü")
print(response)
```

> Ê†πÊçÆËØ¥Êòé‰π¶ÂõûÁ≠îÔºåÊ≠£Á°ÆÂõûÁ≠îÂÜÖÂÆπ

```sh
(lmdeploy) root@intern-studio-030876:~/llamaindex_demo# python llamaindex_RAG.py 
/root/.conda/envs/lmdeploy/lib/python3.10/site-packages/pydantic/_internal/_fields.py:161: UserWarning: Field "model_id" has conflict with protected namespace "model_".

You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.
  warnings.warn(
/root/.conda/envs/lmdeploy/lib/python3.10/site-packages/langchain/_api/module_import.py:92: LangChainDeprecationWarning: Importing ChatMessageHistory from langchain.memory is deprecated. Please replace deprecated imports:

>> from langchain.memory import ChatMessageHistory

with new imports of:

>> from langchain_community.chat_message_histories import ChatMessageHistory
You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/v0.2/docs/versions/v0_2/>
  warn_deprecated(
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:15<00:00,  7.81s/it]
documents number = 579
Âú®MSI X670 GodlikeËäØÁâáÁªÑ‰∏≠ÔºåÊúâ4‰∏™DDR5ÂÜÖÂ≠òÊèíÊßΩ„ÄÇÊîØÊåÅÁöÑÈ¢ëÁéáËåÉÂõ¥‰ªé1R4800 MHzÂà∞6400+ MHz‰∏çÁ≠âÔºåÂÖ∑‰ΩìÊîØÊåÅÁöÑÈ¢ëÁéáÂèñÂÜ≥‰∫éÂ§ÑÁêÜÂô®ÁöÑÂûãÂè∑„ÄÇ

Supported frequencies:
‚Ä¢ 1DPC 1R: 8000+ MHz
‚Ä¢ 1DPC 2R: 6400+ MHz
‚Ä¢ 2DPC 1R: 6400+ MHz
‚Ä¢ 2DPC 2R: 5400+ MHz

Note: The supported frequencies are subject to change as new processors are released. Please refer to www.msi.com for the latest support status.
Query: msi x670 godlike ËäØÁâáÁªÑÊîØÊåÅÂì™‰∫õÊâ©Â±ïÊèíÊßΩÔºü
Answer: MSI X670 GodlikeËäØÁâáÁªÑÊîØÊåÅ3‰∏™PCIe x16ÊèíÊßΩ„ÄÇËøô‰∫õÊèíÊßΩÊîØÊåÅx16/x0/x4Êàñx8/x8/x4ÁöÑÈÖçÁΩÆÔºåÈÄÇÁî®‰∫éAMD Ryzen‚Ñ¢ 7000Á≥ªÂàóÂíåAMD Ryzen‚Ñ¢ 5 8600GÂ§ÑÁêÜÂô®„ÄÇ

Supported PCIe slots:
‚Ä¢ x16/x0/x4 (x8/x8/x4) (for AMD Ryzen‚Ñ¢ 7000 series and AMD Ryzen‚Ñ¢ 5 8600G)
```

![image-20240806134634039](1-4_rag.assets/image-20240806134634039.png)

7. LlamaIndex web

ÂàõÂª∫Êñá‰ª∂

```sh
cd ~/llamaindex_demo
touch app.py
```

```python
import streamlit as st
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.llms.huggingface import HuggingFaceLLM


st.set_page_config(page_title="llama_index_demo", page_icon="ü¶úüîó")
st.title("llama_index_demo")


# ÂàùÂßãÂåñÊ®°Âûã
@st.cache_resource
def init_models():
    embed_model = HuggingFaceEmbedding(
        model_name="./model/bce-embedding-base_v1"
    )
    Settings.embed_model = embed_model

    llm = HuggingFaceLLM(
        model_name = "./model/internlm2_5-1_8b-chat",
        tokenizer_name = "./model/internlm2_5-1_8b-chat",
        model_kwargs = {"trust_remote_code": True},
        tokenizer_kwargs = {"trust_remote_code": True},
        device_map = "auto",
        max_new_tokens = 256,
        generate_kwargs = {"do_sample": True, "temperature": 0.1, "top_k": 50, "top_p": 0.8, "eos_token_id": 2},
    )
    Settings.llm = llm

    documents = SimpleDirectoryReader("./data").load_data()
    print(f"documents number = {len(documents)}")
    index = VectorStoreIndex.from_documents(
        documents = documents,
        embed_model = embed_model
    )
    query_engine = index.as_query_engine(llm = llm)

    print(f"load llm and documents success!")
    return query_engine


# Ê£ÄÊü•ÊòØÂê¶ÈúÄË¶ÅÂàùÂßãÂåñÊ®°Âûã
if 'query_engine' not in st.session_state:
    st.session_state['query_engine'] = init_models()


def greet2(question):
    response = st.session_state['query_engine'].query(question)
    return response


# Store LLM generated responses
if "messages" not in st.session_state.keys():
    st.session_state.messages = [{"role": "assistant", "content": "‰Ω†Â•ΩÔºåÊàëÊòØ‰Ω†ÁöÑÂä©ÊâãÔºåÊúâ‰ªÄ‰πàÊàëÂèØ‰ª•Â∏ÆÂä©‰Ω†ÁöÑÂêóÔºü"}]    

    # Display or clear chat messages
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.write(message["content"])

        
def clear_chat_history():
    st.session_state.messages = [{"role": "assistant", "content": "‰Ω†Â•ΩÔºåÊàëÊòØ‰Ω†ÁöÑÂä©ÊâãÔºåÊúâ‰ªÄ‰πàÊàëÂèØ‰ª•Â∏ÆÂä©‰Ω†ÁöÑÂêóÔºü"}]


st.sidebar.button('Clear Chat History', on_click=clear_chat_history)


# Function for generating LLaMA2 response
def generate_llama_index_response(prompt_input):
    return greet2(prompt_input)

# User-provided prompt
if prompt := st.chat_input():
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.write(prompt)

# Gegenerate_llama_index_response last message is not from assistant
if st.session_state.messages[-1]["role"] != "assistant":
    with st.chat_message("assistant"):
        with st.spinner("Thinking..."):
            response = generate_llama_index_response(prompt)
            placeholder = st.empty()
            placeholder.markdown(response)
    message = {"role": "assistant", "content": response}
    st.session_state.messages.append(message)
```

ËøêË°å

```sh
streamlit run app.py
```

Á´ØÂè£Êò†Â∞Ñ

```sh
ssh -CNg -L 8501:127.0.0.1:8501 root@ssh.intern-ai.org.cn -p 33344
```

![image-20240806134534759](1-4_rag.assets/image-20240806134534759.png)

ÊµèËßàÂô®ËÆøÈóÆÔºåÊàêÂäüËøîÂõûÁªìÊûú

![image-20240806134955202](1-4_rag.assets/image-20240806134955202.png)